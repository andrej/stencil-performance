\section{Benchmarking Various Unstructured Grid Implementations}	\label{sec:results}

In this section we compare the run times of several benchmarks on unstructured and regular grids on three real-world stencils. By applying the same set of stencils on both regular and unstructured grids, we show the effectiveness of the various previously described strategies for grid access and grid storage (see sections \ref{sec:grid-implementations}, \ref{sec:optimizations}). Furthermore, we use key metrics that are made available through the Nvidia profiler in order to better understand what causes the differences in performance.

Real-world stencil applications are applied in a multitude of different scenarios which result in differing problem domain sizes, precision requirements and properties of the stencils themselves. In combination with the different grid access and storage methods described in this report, this leads to a large number of potential setups to be tested. The benchmarks we executed can be characterized by the following properties:

\begin{itemize}
	\item 
		Input/output conditions 
		\begin{itemize}
			\item Domain size (size of the input and output grids)
			\item Required precision (single or double precision)
		\end{itemize}
	\item
		Stencil properties
		\begin{itemize}
			\item \emph{Arithmetic intensity:} The computational effort required to perform the actual output calculations given all input fields
			\item \emph{Number of input and output fields:} How many different values each cell in the grid contains and the stencil operates on
			\item \emph{Depth and number of neighborship dependencies}
		\end{itemize}
	\item
		\emph{Kernel launch configuration:} number of threads, blocks and bytes of shared memory
	\item
		Grid storage implementation properties
		\begin{itemize}
			\item Memory layout and potential regular patterns of stored values
			\item 
				Memory layout of neighborship storage
				\begin{itemize}
					\item Depth of stored neighbor pointers (pointer chasing)
					\item Compression of neighborship table
				\end{itemize}
		\end{itemize}
	\item
		Used grid access implementations
\end{itemize}

In the following, an overview of the benchmark results for the different possible combinations of these properties are given.

\subsection{Benchmarked Stencils and General Benchmark Setup}\label{sec:benchmark-setup}

We benchmark three stencils called \emph{laplace of laplace} (\emph{laplap} for short), \emph{horizontal diffusion} (\emph{hdiff} for short) and \emph{fast waves}. These stencils represent real-world use cases in meteorology applications. Table \ref{tab:benchmarked-stencils} details the values of the main stencil characteristics as listed in the bullet list above.

The definitions of all three stencils are given in terms of a regular grid. Thus, to measure the overhead of using an unstructured grid with indirect addressing, we represent the same regular grid in an unstructured fashion, using two memory layouts, \emph{row-major} and \emph{z-order-curves}. This can be thought of as ``simulating'' an unstructured grid, even if the actual grid structure is entirely regular. Retaining this regularity even in the unstructured representation is required to maintain comparability of the results of the unstructured variants with the regular variants.

\begin{table}											
\begin{tabular}{l c c c c p{4cm}}
	\hline
	Stencil  &  \multicolumn{3}{c}{Neighborhood}  &  Fields  &  Arithmetic Intensity \\
	\cline{2-4}
	&  X  &  Y  &  Z  & \\
	\hline
	\emph{laplap}  &  (-2, 2)  &  (-2, 2)  &  0  &  1  &  Add., Mult., Sub. \\
	\emph{hdiff}  &  (-2, 2)  &  (-2, 2)  &  0  &  2  &  Add., Mult., Sub., Branches \\
	\emph{fastwaves}  &  (0, 1)  &  (0, 1)  &  (-1, 1)  &  9  & Add., Mult., Sub., Div., Branches \\
	\hline
\end{tabular}
\caption{\label{tab:benchmarked-stencils}Stencil Characteristics}
\end{table}

We implement each stencil in several variations as different Cuda kernels; one variant for every one of the grid access optimizations described in section \ref{sec:optimizations}. During the implementation process, the results of each of those variants are verified against a unoptimized reference implementation of the stencil which runs on the CPU to assure correctness of the results.

A regular and an unstructured version of all grid access variants are implemented; one for regular grids and one for unstructured grids. In all variants, the implementation of the actual output value calculations performed in accordance to the stencil definition remains completely unchanged. Only the portions of code responsible for indexing cells and neighbors are swapped to facilitate running the calculations on top of the different grid representations. This is achieved by using \emph{C preprocessor macro definitions} in all places where indexing or neighborship access is required. These macro definitions are then redefined to either use direct or indirect addressing for the respective grids.

The stencils were benchmarked on a \emph{Nvidia Tesla V100} GPU. This GPU implements the \emph{Volta} architecture by Nvidia with compute capability $7.0$. The reported run times are the median of 20 timed kernel runs. Before the first one of the timed kernel runs, an untimed \emph{warm-up run} is performed (see section \ref{sec:foundations}). The GPU is reset using an API call to \texttt{cudaDeviceReset()} between each run in order to flush caches and free device memory from previous runs. The grid values and neighborship relations (for unstructured grids) are stored in unified global memory and are pre-transferred to the GPU and back to the host (\texttt{cudaMemPrefetchAsync()}) before respectively after the kernels are run. Memory transfer from host to device and back is therefore \emph{not} part of the reported run times.

\subsection{Overview of Results}

\begin{table}
	\begin{tabular}{l c c l c r} % c c}		
		\multicolumn{6}{c}{\textbf{Laplace-of-Laplace Stencil}} \\
		\hline
		\hline
		%& & & & \multicolumn{2}{c}{$512\times 512\times 64$ Grid} & \multicolumn{2}{c}{$128\times 128\times 64$ Grid} \\
		%\cline{5-6} %\cline{7-8} 
		Storage & (1) & (2) & Access Strategy  & Runtime & Slowdown \\ %& Runtime & Slowdown \\
		\hline 
		\multicolumn{3}{c}{regular grid baseline} & index variables & $353 \mu s$ & -\\
		\hline
		 row-major & \checkmark & \checkmark & index variables &  $\mathbf{512 \mu s}$ & $\mathbf{45 \%}$ \\
		 row-major & - & \checkmark & index variables & $515 \mu s$ & $46 \%$ \\
		 row-major & \checkmark & - & sliced z-loop & $582 \mu s$ & $65\%$ \\
		 row-major & - & - & sliced z-loop & $608 \mu s$ & $72 \%$ \\
		\hline
		 z-curves & - & \checkmark & naive & $\mathbf{528 \mu s}$ & $\mathbf{50 \%}$ \\
		 z-curves & \checkmark & \checkmark & index variables & $532 \mu s$ & $51 \%$ \\
		 z-curves & \checkmark & - &  z-loop & $546\mu s$ & $55 \%$ \\
		 z-curves & - & - & z-loop & $603 \mu s$ & $71 \%$ \\
		
		\hline
		\hline \\
		\multicolumn{6}{c}{\textbf{Horizontal Diffusion Stencil}} \\
		\hline
		\hline
		Storage & (1) & (2) & Access Strategy  & Runtime & Slowdown \\
		\hline
		\multicolumn{3}{c}{regular grid baseline} & index variables & $546 \mu s$ & - \\
		\hline
 		 row-major & - & \checkmark & naive & $\mathbf{683 \mu s}$ & $\mathbf{25 \%}$ \\
		
		 row-major & \checkmark & \checkmark & index variables & $731 \mu s$ & $34 \%$ \\
		
		 row-major & \checkmark & - & index variables & $804\mu s$ & $47\%$ \\
		 &   & &  shared (tie) & $804\mu s$ & $47\%$ \\
		 row-major & - & - & naive & $845 \mu s$ & $55 \%$ \\
		\hline
		 z-curves & - & \checkmark & naive & $\mathbf{710 \mu s}$ & $\mathbf{30 \%}$ \\
		 z-curves & \checkmark & \checkmark & index variables & $741 \mu s$ & $36 \%$ \\
		 z-curves & \checkmark & - & index variables & $820\mu s$ &  $50 \%$ \\
		 z-curves & - & - & shared & $841 \mu s$ & $54 \%$ \\
		
		\hline
		\hline\\
		\multicolumn{6}{c}{\textbf{Fastwaves Stencil}}\\
		\hline
		\hline
		Storage & (1) & (2) & Access Strategy  & Runtime & Slowdown \\
		\hline
		\multicolumn{3}{c}{regular grid baseline} & naive & $2298 \mu s$ & - \\
		\hline
		row-major & & - & naive & $\mathbf{2400\mu s}$ & $\mathbf{4.4 \%}$ \\
		row-major & & \checkmark & naive & $2433\mu s$ & $5.9 \%$ \\
		\hline
		z-curves & & - & naive & $\mathbf{2426\mu s}$ & $\mathbf{5.6 \%}$ \\
		z-curves & & \checkmark & naive & $2438\mu s$ & $6.1 \%$ \\
		\hline\hline
	\end{tabular}
	\begin{enumerate}[label=(\arabic*)]
		\item Pointer Chasing? (Checkmark if only direct neighbors are stored, no neighbors-of-neighbors, and pointer chasing occurs.)
		\item Compressed? (Checkmark if neighborship table was compressed.)
	\end{enumerate}
	\caption{\label{tab:overview} Fastest access strategy, runtimes (median of 20 runs) and relative slowdown given the grid storage type for the three benchmarked stencils on a $512\times 512\times 64$-sized grid of doubles.}
\end{table}

Table \ref{tab:overview} lists the fastest storage/access-combinations for all stencils and reports their runtimes and overhead. Across all executed benchmarks, we measured that the main cost of switching to an unstructured grid is a roughly a constant time penalty. On a $512\times 512\times 64$ grid, the additional time required by the unstructured kernels is between $137 \mu s$ and $159 \mu s$ for the fastest variants of the horizontal diffusion and Laplace-of-Laplace stencils. The latency in the computationally more complex fastwaves stencil is lower at $102 \mu s$ (in the best case), as it only accesses four neighbors. It is likely that the cost paid when switching to indirect addressing is mainly due to the latency of the neighborship table accesses which must occur before any computations can be done, and that these measured overheads quantify that latency.

The relative slowdowns are in the range of $4\%$ to $71\%$. The biggest differences in these relative slowdowns are observed due to differences between the stencils. As the overhead is largely constant, simpler stencils with less computation time spent per cell suffer more. For the most simple stencil, Laplace-of-Laplace (\emph{laplap}), relative slowdowns are largest with values of roughly $45\%$ to $71\%$. The more complex fastwaves stencil, which accesses nine different fields (compared to the one field of the \emph{laplap} stencil) only suffers from slowdowns in the $5\%$-area. The medium-intensity horizontal diffusion sits in the middle with its $25\% - 55\%$ range. 

In section \ref{sec:res-storage} and \ref{sec:res-access}, we explore the performance of the storage and access methods used. The chosen grid access strategy can have a high impact on the runtimes. Yet, no access strategy clearly dominates all others. Which implementation is fastest depends on the combination of access and storage, as well as on the stencil. Surprisingly, the \emph{naive} access strategy is competitive, and at times better than the optimized variations. The \emph{idxvar} optimization always performs similarly or slightly better. The optimized \emph{z-loop}, \emph{sliced z-loop} and the \emph{shared} access strategy did in most cases not pay off.

For both the horizontal diffusion and Laplace-of-Laplace stencils, compressing the neighborship table provides a significant benefit. However, due to the additional memory lookup required and unavoidable uncoalesced accesses to the compressed neighborship table, the advantage is not as large as one might at first expect.

Most results presented in this section focus on grids of size $512\times 512\times 64$. However, variations in the problem domain also lead to interesting observations. As the grids are still structured in the Z-dimension, changing the size of dimension and measuring the accompanying slowdowns reveals how the different implementations are able to make use of the regularity. In much smaller grids, the runtimes change in more hard-to-predict manners. At smaller sizes, the cache can be used more effectively. Generally, the relative slowdown in smaller grids also becomes smaller. Section \ref{sec:res-size} gives some insight into how the performance relates to the input domain size.

The choice of the right number of threads when launching the kernels is especially important. Depending on the implementation, a different number of threads, or a different arrangement of the threads is better. In section \ref{sec:res-blocksize} the effect of the block sizes is evaluated and explained.

\begin{verbatim}
% TODO 
% - Overview graph of only fastest variant per stencil
\end{verbatim}

\subsection{Effect of Grid Storage} \label{sec:res-storage}

In section \ref{sec:grid-implementations}, we described different approaches to storing an unstructured grid. Four storage approaches were benchmarked as a result of varying the following two properties: First, the depth of the neighborship table, i.e. whether only neighbors (\emph{chasing} variant) or also neighbors-of-neighbors (\emph{non-chasing} variant) were stored. Second, whether the neighborship table was \emph{compressed} or \emph{uncompressed}.

\begin{figure}
	% ax=u.barplot(df64mins[(df64mins["stencil"]=="hdiff")&df64mins["z-curves"]&(df64mins["size-x"]>=128)], grp=u.col.stencil+u.col.size, cat=u.col.storage, tickrot=0, y="rel")
	% ax.set_ylim(bottom=1)
	% fig=u.plotdone()
	% u.plotsave("report/img/hdiff-z-curves-sizes-storage.pdf", fig)
	\comment{
	      stencil  size-x  size-y  size-z  unstructured        variant  z-curves  no-chase   comp  threads-x  threads-y  threads-z       min       max       avg    median
	11327   hdiff     128     128      64          True         idxvar      True     False  False         64          1          8   50000.0   53000.0   51000.0   51000.0
	12079   hdiff     128     128      64          True         idxvar      True     False   True         32          1         16   52000.0   56000.0   53000.0   53000.0
	10516   hdiff     128     128      64          True         idxvar      True      True  False         32          2          8   47000.0   50000.0   48000.0   49000.0
	10880   hdiff     128     128      64          True          naive      True      True   True         32          1          8   48000.0   51000.0   50000.0   50000.0
	32073   hdiff     512     512      64          True         idxvar      True     False  False         64          1          8  801000.0  828000.0  817000.0  820000.0
	31623   hdiff     512     512      64          True         idxvar      True     False   True        128          1          2  734000.0  746000.0  741000.0  741000.0
	32178   hdiff     512     512      64          True  idxvar-shared      True      True  False         32          1         16  819000.0  854000.0  838000.0  841000.0
	31541   hdiff     512     512      64          True          naive      True      True   True        128          1          2  707000.0  721000.0  713000.0  710000.0
	}
	\includegraphics[scale=0.75]{hdiff-z-curves-sizes-storage.pdf}
	\caption{\ref{fig:hdiff-sizes-storage} Relative slowdowns for the horizontal diffusion stencil on a small and a larger grid. The fastest access variant is plotted for each bar, which is one of \emph{index variables}, \emph{naive} and \emph{idxvar-shared} for the shown benchmarks. This example for the horizontal diffusion stencil is representative for the other two benchmarks as well, where the slowdowns follow a similar pattern. (Note that in the fastwaves stencils, pointer chasing is not applicable as there only direct neighbors are accessed.)}
\end{figure}

\subsubsection{Pointer Chasing versus Explicit Neighbor-of-Neighbor Storage}
\label{sec:results-chasing}

Two of the three benchmarked stencils, Laplace-of-Laplace and Horizontal Diffusion, access neighbors beyond the directly face-touching cells (neighbors of neighbors). For those stencils, two implementations were benchmarked: First, we assessed the performance when only direct neighbors are explicitly stored. This approach requires \emph{pointer chasing}: In order to access the neighbor of a neighbor, two sequential lookups become necessary. As the second lookup can only be started when the first one completed, a higher latency may occur in this variant. Therefore, we then also tested variants in which the neighbors-of-neighbors were explicitly stored in memory, reducing the number of required memory lookups for those cells from two to one. This comes at the cost of a higher memory footprint, however.

Generally we observe the following trend: If the grid is small or compressed, pointer chasing becomes an issue (and thus non-chasing variants profit). In larger uncompressed grids, the latency of the two lookups required in pointer chasing for neighbors-of-neighbors can be effectively hidden. This is visible in figure \ref{fig:hdiff-sizes-storage}, which outlines the differences of the storage approaches for the fastest respective variant of the horizontal diffusion stencil on grids of different sizes. 

In all cases a higher total number of device memory and L2 cache reads is observed in the \texttt{nvprof} metrics for the non-chasing variants. This is to be expected, as non-chasing variants need to store and read more information from a larger number of different addresses. The L1 cache hit rate lowers by around $6\%$ in both large and small grids when pointers to neighbors-of-neighbors are explicitly stored. In large grids, the L2 cache hit rate also drops dramatically; for smaller grids, it is unaffected and remains high. Thus, the additional neighbor-of-neighbor information stored hurts cache locality more in larger grids, putting non-chasing variants at a disadvantage there. This is due to the fact that larger grids require more of this information, while the cache size remains constant -- consequently, some data has to be evicted from the cache. This does not happen as often in a smaller grid. The variants that perform pointer chasing experience a higher percentage of stalls due to memory dependencies due to the second lookup depending on the result of the first for neighbor-of-neighbor reads.  We conclude that non-chasing storage is beneficial to small grids, where it is effective in reducing latency, and unfavorable for larger grids due to the caused cache evictions (especially in the L2 cache), that actually increase the latency of neighbor acceses. Table \ref{tab:chasing} shows an overview of the metrics discussed in this paragraph for an exemplary stencil configuration of the horizontal diffusion stencil.

\begin{table}
	\comment{chase-v-no-chase.txt}
	\begin{tabular}{l l l l l l}
		\hline
		\textbf{Size} & \textbf{Chasing?} & \textbf{\texttt{tex\_cache\_hit\_rate}} & \textbf{\texttt{l2\_tex\_hit\_rate}} & \textbf{\texttt{stall\_memory\_dependency}} & \textbf{Runtime} \\
		\hline
		$128\times 128\times 64$ & - & 			$73\%$ & $68\%$ & $59\%$ & $\mu s$ \\
		$128\times 128\times 64$ & \checkmark & $67\%$ & $72\%$ & $54\%$ & $\mu s$ \\
		$512\times 512\times 64$ & - & 			$68\%$ & $58\%$ & $74\%$ & $\mu s$ \\
		$512\times 512\times 64$ & \checkmark & $63\%$ & $37\%$ & $60\%$ & $\mu s$ \\
		\hline
	\end{tabular}
	\caption{\label{tab:chasing} Selected metrics for runs of the horizontal diffusion stencil on different types of grids (all of them in z-curves layout). The fastest found variant and launch configuration was chosen for each configuration seperately.}
\end{table}

\subsubsection{Advantage of Compressed Neighborship Tables}

Across all stenils except fastwaves, and across all access implementations, compressing the neighborship table improves performance for large enough grids. Table \ref{tab:compression} shows the distribution of neighborship table entries attained by the trivial compression scheme that we implemented is on a $512\times 512\times 64$-sized grid. If the values of the grid are laid out in row-major fashion, compression is especially effective. In this case, only the elements in the halo have different neighborship pointers; all others share the top entry. Using a z-curves layout, the distribution of neighborship table accesses is more evenly spread out. However, the concentration of several frequently occuring patterns is still able to provide a runtime benefit.

\begin{table}
	\begin{tabular}{l l l l l l}
		\hline
		Compressed? & Chasing? & Storage & \# Entries & Ratio\textsuperscript{*} & Freq.\textsuperscript{\dag} \\
		\hline
		- & \checkmark & both & $1,048,576$ & $1$ & $<1\%$\\
		- & - & both & $3,145,728$ & $1$ & $<1\%$\\
		\checkmark & \checkmark & row-major & $2054$ & $0.00078$ & $97.7\%$ \\
		\checkmark & \checkmark & z-curves & $2435$ & $0.00093$ & $20.3\%$ \\
		\checkmark & - & row-major & $4093$ & $0.00156$ & $96.9\%$ \\
		\checkmark & - & z-curves & $5299$ & $0.00202$ & $8.1\%$ \\
		\hline
	\end{tabular}
	\caption{\label{tab:compression} Properties of neighborship table before and after compression for different types of grids. \textsuperscript{*}Ratio of number of compressed neighborship table entries to number of uncompressed neighborship table entries. \textsuperscript{\dag}The frequency given is the frequency of the most common neighborship table entry. }
\end{table}

\begin{verbatim}
% TODO
% - Relative advantage of compression for all variants/combinations
% - Why not larger advantage? -> Metrics to back up coalescing explanation/latency of first prototype lookup and second lookup is non-coalesced
% - Why uncompressed faster for fastwaves?

% mention fewer memory dram transactions/bytes in compressed variants
% mention higher hit rate for large grids, smaller hit rate for small
\end{verbatim}

\subsection{Effect of Access Strategy} \label{sec:res-access}

In section \ref{sec:optimizations}, we discussed various optimizations for accessing such grids in a stencil application. Here, we compare the performance of these approaches. Figure \ref{fig:storage-access} gives an overview of the possible storage/access-combinations.

\begin{figure}
	%\includegraphics[scale=0.75]{laplap-zcurves-grouped-variants.pdf} % ./plot.py -i results/ultimate.csv -p grouped --size $((512*512*64)) --bar-groups variant --color storage --marker storage --z-curves z-curves --stencil laplap --title "laplap" -o results/variants/laplap-zcurves-grouped-variants.pdf --exclude laplap-regular-naive --scale-max 1000
	\includegraphics[scale=0.75]{laplap-zcurves-grouped-storage.pdf} % ./plot.py -i results/ultimate.csv -p grouped --size $((512*512*64)) --bar-groups storage --color variant --marker variant --z-curves z-curves --stencil laplap --title "laplap" -o results/variants/laplap-zcurves-grouped-storage.pdf --exclude laplap-regular-naive --scale-max 1000
	\includegraphics[scale=0.75]{hdiff-zcurves-grouped-variants.pdf} % ./plot.py -i results/ultimate.csv -p grouped --size $((512*512*64)) --bar-groups variant --color storage --marker storage --z-curves z-curves --stencil hdiff --title "hdiff" -o results/variants/hdiff-zcurves-grouped-variants.pdf --exclude hdiff-regular-naive --scale-max 1200
	\includegraphics[scale=0.75]{fastwaves-zcurves-grouped-variants.pdf} % ./plot.py -i results/ultimate.csv -p grouped --size $((512*512*64)) --bar-groups variant --color storage --marker storage --z-curves z-curves --stencil fastwaves --title "fastwaves" -o results/variants/fastwaves-zcurves-grouped-variants.pdf --exclude fastwaves-regular-naive --scale-max 4000
	\caption{\label{fig:storage-access} Median runtimes across 20 runs for stencils on a $512\times512\times 64$-sized unstructured grid (z-curves layout). The fastest blocksize was chosen and reported per variant. The time reported for the regular grid is the fastest of all implemented regular variants. Results for row-major-stored grids are similar. Note that for the \emph{fastwaves} stencil, only direct neighbors are accessed; pointer chasing thus never occurs.}
\end{figure}

\subsubsection{\emph{Naive} and \emph{Idxvar} Access Strategies}
The \emph{naive} and \emph{idxvar} (index variables) access strategies are the fastest for all three stencils. In some cases, the \emph{naive} access strategy is faster than the \emph{idxvar} variant by a very slight margin. This is the case, for example, in the fastwaves stencil or the horizontal diffusion stencil executed on a non-chasing, compressed grid. This result might appear surprising at first, as the naive strategy repeatedly performs memory lookups for the same neighborship pointers. The index variables approach tries to reduce these reduntant lookups by storing the needed neighborship indices in temproary variables at the start of the kernel execution. However, this adds some overhead.

For one, the index variables variant for the fastwaves kernel leads to a higher register use when compiled (44 registers for \emph{idxvar} versus 42 for \emph{naive}). As registers are shared across threads of a block, this reduces the theoretical amount of threads that can be launched concurrently. As this difference is not present in the other stencils, this does not appear to be the main reason for the advantage, however. There are two more likely explanations for the advatnage of the \emph{naive} variant: First, access to the same memory location in the \emph{naive} kernel becomes as cheap as a register access after a neighborship pointer are read for the first time, as it will be held in the L1 cache. Reaccessing the same memory location helps keep the values fresh in all caches. As such, when a different thread requires the same memory location from the neighborship table, it is more likely to be in cache in the \emph{naive} variant. Second, the naive approach has better \emph{instruction level parallelism}. Because the index variables are assigned at the start of the \emph{idxvar} variant, all loads to the neighborship table are gathered in one sequence and must be executed before anything else. In the \emph{naive} approach, on the other hand, neighborship pointers are (re-)loaded only when needed in calculations. Thus, after one neighbor has been loaded, useful calculations using its value may already be performed. Table \ref{tab:fastwaves-naive-idxvar-metrics} details profiler metrics that support these claims.

\begin{table}
	\begin{tabular}{l l l}
		\hline
		Metric & \emph{naive} & \emph{idxvar} \\
		\hline
		Run time & $\mathbf{2438\mu s}$ & $2543\mu s$ \\
		Global load transactions & $126264460$ & $124235165$ \\
		L1 transactions & $38077396$ & $34593863$ \\
		L2 transactions & $56635611$ & $57264861$ \\
		Device Memory transactions & $36825619$ & $37327310$ \\
		Executed Instructions Per Cycle & $\textbf{0.522}$ & $0.497$ \\
		\hline
	\end{tabular}
	\caption{\label{tab:fastwaves-naive-idxvar-metrics}Selection of metrics for \emph{fastwaves} stencil run on a $512\times 512\times 1$-sized unstructured grid (z-curves layout, compressed neighborship table) with $32\times 1\times 8$ threads, which is the fastest block size for both variants. The \emph{naive} implementation is faster, even though it redoes the index lookups in the neighborship tables. The total global transaction count shows that the \emph{idxvar} variant successfully reduces the number of neighborship lookups performed. However, the cache transactions (L1 and L2) indicate that the naive variant keeps cache contents fresher and results in more cache hits. This is evidenced by the lower number of actual device memory reads in the naive approach, despite the idxvar approach requesting less memory.}
\end{table}

\subsubsection{\emph{Shared} Access Strategy}

\begin{verbatim}
% mention it is faster for no-chase, uncompressed grid storage
% -> explicit shared memory usage better than relying on cache when there is a very large number of neighbors

% TODO
% - shared is almost same as idxvar due to caches
% - overhead of synchronization makes it slightly slower
% - back this up with metrics
\end{verbatim}

\subsubsection{\emph{Z-loop} and \emph{Z-loop-sliced} Access Strategies}

\begin{verbatim}
% TODO
% - reiterate what the supposed advantage of z loop is
% - Why is it faster for uncompressed variants, why slower for compressed?
%    -> Back up with metrics!
\end{verbatim}

\subsection{Effect of Problem Domain Size and Precision} \label{sec:res-size}

Generally, the relative slowdowns decrease with increasing domain size in the Z-dimension ($512\times 512\times 1$ up to $512\times 512\times 64$). This can be explained by the regularity of the grid in the Z-dimension, which some of the optimized implementations make use of. Theoretically, only one lookup to determine neighboring indices is required for all cells in a grid with equal X- and Y-coordinates. Even the unoptimized variants profit from this regularity, as neighborship information appears to be mostly kept in the caches.

\begin{figure}
	\makebox[\textwidth]{
	\includegraphics[width=0.3\paperwidth]{laplap-512x512-size-z.pdf}
	\includegraphics[width=0.3\paperwidth]{hdiff-512x512-size-z.pdf}
	\includegraphics[width=0.3\paperwidth]{fastwaves-512x512-size-z.pdf}
	}
	\caption{\label{fig:results-512-sizes}Runtimes relative to stencil on regular grids sized $512\times 512\times 1$ up to $512\times 512\times 64$}
\end{figure}

\begin{verbatim}
% TODO
% - effect of Z-size increase
%   -> graph per variant (choose best-looking storage strategy if all similar)
%   -> reiterate how this shows that z regularity is made use of in certain variants
%   -> point to latency for first neighbor lookups that cannot be hidden
% - effect of XY-size change
%   -> smaller sizes 128x128x64
%   -> larger sizes 512x512x64
% - effect of precision
%   -> grouped bar graph double vs single; keep short
\end{verbatim}

\subsection{Optimal Block Size} \label{sec:res-blocksize}

\subsubsection{Background}

One of the main determining factors for performance of a kernel is its launch configuration, which includes its grid size (number of blocks) and block size (number of threads, see section \ref{sec:hardware}). Cuda provides the option to specify block sizes in three dimensions, providing each thread with a X-, Y- and Z-index. In our benchmarks, we have tested all possible combinations of block sizes in steps of powers of two from 32 up to 512.

In order to cover the entire problem domain (which remains constant in size), a decrease in number of threads is always accompanied by an increase in the number of blocks (grid size), and vice versa. Owing to the way we map thread indices onto memory indices (X thread index maps onto memory index directly), thread sizes in the X-dimension of less than $32$ are highly inefficient. They lead to non-coalescing memory accesses. The X-dimension of the benchmarked block sizes is therefore always at least $32$.

\subsubsection{Overview}

The optimal \emph{total number} of threads depends mostly on the grid access implementation employed. In general, the same patterns can be observed for all three tested stencils and for all tested grid memory storage implementations: When using the \emph{naive}, \emph{idxvar} or \emph{shared} grid access strategies, it is best to have a high number of threads in total (256-512). The \emph{kloop} and \emph{kloop-sliced} thrive on a lower number of threads due to occupancy concerns. An exemplary overview of the runtimes as a function of total block size is given in figure \ref{fig:blocksizes-overview}.

\begin{figure}
	\includegraphics[scale=0.75]{laplap-z-curves-no-comp-chase-threads-prod.pdf}
	\caption{\label{fig:blocksizes-overview} Overview of the runtime for different implementations of the \emph{laplap} stencil on a $512\times 512\times 64$-sized domain (unstructured grid, pointer chasing, z-curves layout, uncompressed neighborship table) as a function on different total block sizes (product of X-, Y- and Z-blocksize). Results for other stencils and variants are of similar shape.}
	% ./plot.py -i results/ultimate.csv --size 16777216 -p line -x threads-prod --stencil laplap --z-curves z-curves --chase chase --comp no-comp  --marker variant --scale-min 400 --scale-max 1200 --plot-size 5 4
\end{figure}

The best \emph{shape} of the block size vector changes with the used grid storage strategy. Using \emph{uncompressed} neighborship tables, it is best for the \emph{naive}, \emph{idxvar} and \emph{shared} access strategies to have 32 or 64 threads in X-dimension and the rest in Z-dimension. Those implementations profit of more Z-threads because of the regularity of the grid in the Z-dimension. The \emph{non-chasing} variants profit even more of additional threads in the Z-dimension. This is due to the fact that in non-chasing variants, more neighborships are stored in the table; cache entries are thus evicted faster if many different X- and Y-coordinates are accessed. Switching to \emph{comepressed} neighborship tables, additional threads in the Z-dimension are of no use to any of the access strategies -- the reduced number of neighborship table entries after compression is accessed often enough to remain in cache independent of the Z-dimension of the block size. See figure \ref{fig:blocksizes-z} for a comparision of how changes in the Z-dimension of the blocksize vector translate to runtime changes. Whether the grid is stored as row-major or using z-order curves does not impact the required block sizes.

\begin{figure}
	\includegraphics[scale=0.75]{laplap-zcurves-idxvar-threads-z.pdf} % ./plot.py -b unstr -i results/ultimate.csv --stencil laplap --variant idxvar --z-curves z-curves --size $((512*512*64)) --color storage --marker storage -p line -x threads-z -o results/blocksizes/laplap-zcurves-idxvar-threads-z.pdf --scale-max 1500
	\caption{\label{fig:blocksizes-z}Fastest runtimes for each fixed number of threads in the Z-dimension (other dimensions free) for the \emph{laplap} stencil on a $512\times 512\times 64$-sized domain (unstructured grid, row-major storage) for compressed and uncompressed variants, with and without pointer chasing.}
\end{figure}

\subsubsection{Optimal Block Sizes per Access Strategy}

\paragraph{\emph{Naive}, \emph{Idxvar} and \emph{Shared} Grid Access Strategies}
In order to always cover the entire problem domain, the number of blocks $b$ in each dimension in the \emph{naive}, \emph{idxvar} and \emph{shared} grid access strategies is chosen as a function on the problem size $d$ and the chosen number of threads $t$ as follows:
$$b = \begin{pmatrix}\left\lceil\frac{d_x}{t_x}\right\rceil & \left\lceil\frac{d_y}{t_y}\right\rceil & \left\lceil\frac{d_z}{t_z}\right\rceil\end{pmatrix}^\top$$

For the \emph{naive}, \emph{idxvar} and \emph{shared} implementations, a high number of threads per block is beneficial to performance. For grids using an uncompressed neighborship table, increasing the number of threads in the Z-dimension especially improves performance.

For the \emph{naive} and \emph{idxvar} strategies, the advantage of multiple threads in Z (in grids using uncompressed neighborship tables) can be explained through the regularity of the unstructured grid in Z-direction: Cells with identical X and Y coordinates share the same entry in the neighborship table. By having multiple threads in a block operate on different Z-levels, caching of this neighborship table entry becomes very effective. Evidence for this presumed reason for the speedup is given by two facts: One, of all block size combinations tested, the ones with more threads in the Z-dimension perform best. Two, the Nvidia profiler reports higher cache hit rates if the number of threads in Z-dimension is increased. As an example of this for the \emph{idxvar} access strategy, see table \ref{tab:laplap-blocksize-metrics}.

These effects are strongest for grids that also store neighbors-of-neighbors in the neighborship table (no pointer chasing). As this leads to a larger number of entries in the neighborship table that are stored and accessed, entries may be evicted from the cache more quickly. More threads in the Z-dimension, which access the same neighborship table entries, prevent this.

At the opposite end of the spectrum lie the grids stored using compressed neighborship tables. These tables are much smaller, and the same few entries are accessed in almost all threads. Because of this, neighborship table entries remain in cache no matter the shape of the block size vector, and additional threads in the Z-dimension bring no benefit. Cache locality for the accessed values is more important here; depending on the used layout for the values (z-order-curves or row-major) this means adding additional threads in X (z-curves, spatial locality given), or X and Y (row-major, adding another thread in Y gives some spatial locality).

\begin{table}
%	\begin{tabular}{l l l l l}
%		Blocksize & \texttt{global\_hit\_rate} & \texttt{tex\_cache\_hit\_rate} & \texttt{l2\_tex\_read\_hit\_rate} & \texttt{l2\_tex\_hit\_rate} \\
%		\hline
%		$32\times 1\times 1$ & $45.42\%$ & $44.39\%$ & $73.86\%$ & $67.69\%$ \\
%		$32\times 1\times 2$ & $58.83\%$ & $57.96\%$ & $76.60\%$ & $68.12\%$ \\
%		$64\times 1\times 2$ & $62.25\%$ & $66.21\%$ & $69.99\%$ & $60.36\%$ \\
%		$64\times 1\times 4$ & $72.33\%$ & $71.72\%$ & $72.77\%$ & $60.81\%$
%	\end{tabular}
	\begin{tabular}{l l l l l}
		Blocksize & \texttt{tex\_cache\_hit\_rate} & \texttt{l2\_tex\_hit\_rate} \\
		\hline
		$32\times 1\times 1$ & $44.39\%$ & $67.69\%$ \\
		$32\times 1\times 2$ & $57.96\%$ & $68.12\%$ \\
		$64\times 1\times 2$ & $66.21\%$ & $60.36\%$ \\
		$64\times 1\times 4$ & $71.72\%$ & $60.81\%$
	\end{tabular}
	\caption{\label{tab:laplap-blocksize-metrics} Cache hit rates for different block sizes for the \emph{laplap} stencil using a \emph{idxvar} grid access strategy on a $512\times 512\times 64$-sized domain (unstructured grid, pointer chasing, stored in z-curves layout, uncompressed neighborship table)}
\end{table}

By the design of the \emph{shared} access strategy, a speedup is supposed to be attained with a larger number of Z-threads; if there are more threads in Z-dimension within a block, more sharing of neighborship relations through shared memory can take place. These implementations indeed profit from more threads through shared memory in much the same way as \emph{naive} and \emph{idxvar} variants profit from more threads through the cache in the uncompressed grids.

\paragraph{\emph{Z-loop} and \emph{Z-loop-sliced} Grid Access Strategies}
Contrary to the other stencils, the \emph{z-loop} and \emph{z-loop-sliced} grid access strategies suffer from a high total thread count. As both of these strategies require one thread to perform calculations for multiple cells on different Z-levels, the total number of blocks and threads required to cover the entire grid is smaller. On the largest tested grid size ($512\times 512\times 64$), this leads to occupancy issues: As threads stall, there is not enough work can be scheduled to certain streaming multiprocessors to hide that latency. All threads inside a block are required to be executed on the same SM. Having more threads inside a block thus gives less blocks that can be scheduled onto stalled SMs. Smaller block sizes, on the other hand, give the scheduler a larger pool of blocks to choose from when trying to hide latency. Compare table \ref{tab:laplap-blocksize-occupancy} for an example of how a too large block size negatively impacts the \emph{kloop} implementation of the exemplary \emph{laplap} benchmark.

\begin{table}
	\begin{tabular}{l l l l l}
		Variant & Blocksize & Runtime & \texttt{achieved\_occupancy} & \texttt{issue\_slot\_utilization} \\
		\hline
		idxvar & $512$ & $783\mu s$ & $85\%$ & $19\%$ \\
		idxvar & $128$ & $844\mu s$ & $\mathbf{92\%}$ & $18\%$ \\
		idxvar & $32$  & $986\mu s$ & $48\%$ & $16\%$ \\
		z-loop & $512$ & $796\mu s$ & $25\%$ & $10\%$\\
		z-loop & $128$ & $\mathbf{546\mu s}$ & $42\%$ & $13\%$ \\ 
		z-loop & $32$  & $584\mu s$ & $41\%$ & $12\%$
	\end{tabular}
	\caption{\label{tab:laplap-blocksize-occupancy} Runtimes and occupancy metrics of \emph{idxvar} and \emph{z-loop} variant implementations of a \emph{laplap} stencil on an unstructured grid (size $512\times 512\times 64$, uncompressed neighborship table, z-curve memory layout, pointer chasing) for a selection of block sizes. The following block shapes were used: $512 = 256\times 2\times 1$, $128 = 64\times 2\times 1$ and $32 = 32\times 1 \times 1$. This highlights the occupancy issues the z-loop variant faces with too large block sizes. Note how the occupancy drops with increased number of threads for the z-loop variant, and how the runtime increases with it.}
\end{table}