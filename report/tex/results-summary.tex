\section{Summary of Results}

Across all executed benchmarks, we measured that the main cost of switching to an unstructured grid is roughly a constant time penalty. On a $512\times 512\times 64$ grid, the additional time required by the unstructured kernels is between $137 \mu s$ and $159 \mu s$ for the \emph{hdiff} and \emph{laplap} stencils in the best case, respectively. The additional cost of the fastest unstructured implementation in the computationally more complex fastwaves stencil is lower at $102 \mu s$. We suppose this is because it only accesses four neighbors. It is likely that the cost paid when switching to indirect addressing is mainly due to the latency of the neighborship table accesses which must occur before any computations can be done and that these measured overheads quantify that latency.

Setting these overheads in relation to the fastest regular implementation runtimes, we get relative slowdowns are in the range of $4\%$ to $71\%$. The biggest differences in these relative slowdowns are observed due to differences between the stencils. As the overhead is largely constant, simpler stencils with less computation time spent per cell suffer more. For the most simple stencil, Laplace-of-Laplace (\emph{laplap}), relative slowdowns are largest with values between $45\%$ and $71\%$. The more complex fastwaves stencil, which accesses nine different fields (compared to the one field of the \emph{laplap} stencil) only suffers from slowdowns in the $5\%$-area. The medium-intensity horizontal diffusion sits in the middle with its $25\% - 55\%$ range. 

Table \ref{tab:overview} summarizes the fastest storage/access-combinations for all stencils and reports their runtimes and overhead. We explored the differences that the employed storage and access strategies make in sections \ref{sec:res-storage} and \ref{sec:res-access} respectively. For low- to medium-intensity stencils operating on large grids, we recommend using the \emph{idxvar} access strategy on a grid stored in \emph{compressed, non-chasing} fashion, as this was the fastest combination across our benchmarks. 

We observed that performance differences also depend largely on the problem domain size. Most results presented focussed on grids of size $512\times 512\times 64$. In smaller grids, deliberate optimization attempts often proved to be ineffective. Some of our optimizations introduced small, one-time overheads intended to pay off later; this is only effective in larger sized-grids.

The choice of the right number of threads when launching the kernels is especially important. Depending on the implementation, a different number of threads, or a different arrangement of the threads is better. In section \ref{sec:res-blocksize} the effect of the block sizes is evaluated and explained.