\section{Effect of Access Strategy in Stencil Implementation} \label{sec:res-access}

Before benchmarking, all stencils had to be reimplemented in a way that supports unstructured grids. In this section, we explore the performance implications of the access variants described in section \ref{sec:optimizations}, starting from the \emph{naive} variant and moving to the contrived optimizations.

\subsection{\emph{Naive} and \emph{Idxvar} Access Strategies}

\begin{table}
	\begin{center}
    \begin{tabular}{l l l}
        \hline
        \textbf{Metric} & \textbf{\emph{naive}} & \textbf{\emph{idxvar}} \\
        \hline
        \hline
        Run time & $\mathbf{2438\mu s}$ & $2543\mu s$ \\
        Global load transactions & $126,264,460$ & $124,235,165$ \\
        L1 transactions & $38,077,396$ & $34,593,863$ \\
        L2 transactions & $56,635,611$ & $57,264,861$ \\
        Device Memory transactions & $36,825,619$ & $37,327,310$ \\
        Executed Instructions Per Cycle & $\textbf{0.522}$ & $0.497$ \\
        \hline
    \end{tabular}
	\end{center}
    \caption{\label{tab:fastwaves-naive-idxvar-metrics}Selection of metrics for the \emph{fastwaves} stencil run on a $512\times 512\times 64$-sized unstructured grid (\emph{z-curves} memory layout with \emph{double} precision, \emph{compressed} and \emph{chasing} neighborship table) with $32\times 1\times 8$ threads, which is the fastest block size for both displayed access variants. The \emph{naive} implementation is faster, even though it redoes the index lookups in the neighborship tables. The total global transaction count shows that the \emph{idxvar} variant does reduce the number of lookups performed. However, the cache transactions (L1 and L2) indicate that the \emph{naive} variant keeps cache contents fresher, which results in more cache hits. This is evidenced by the lower number of actual device memory reads in the naive approach, even though more memory is requested than in the \emph{idxvar} approach.}
\end{table}

The \emph{naive} strategy was the first we implemented. At each neighbor access in the reference regular-grid-stencil, an indirect lookup into the neighborship table is inserted for the unstructured variant. As such, this implementation performs repeated redundant lookups for the same neighborship table entries if the same neighbor is accessed multiple times in one thread. Our initial tests occurred on a $512\times 512\times 64$-sized grid (\emph{uncompressed}, \emph{chasing}, \emph{row-major}), and using \emph{naive} access strategy, we observed slowdowns of $2.07x$, $1.54x$ and $1.044x$ (compared to the fastest regular implementations) for the \emph{laplap}, \emph{hdiff} and \emph{fastwaves} stencils, respectively.

We then moved to a straightforward optimization, the \emph{idxvar} variant. This approach tries to eliminate redundant lookups by storing the needed neighborship indices in temporary variables at the start of the kernel execution. In the $512\times 512\times 64$ grid (\emph{uncompressed}, \emph{chasing}, \emph{row-major}), this reduced the overheads to $1.77x$ and $1.47x$ for the \emph{laplap} and \emph{hdiff} stencils.

Comparing the \emph{naive} and \emph{idxvar} strategies to the more optimized variants (discussed below), we observed that caching of neighborship table entries plays a paramount role in unstructured stencil performance. We discovered that neighborship table lookups become fairly efficient once cached. Due to their simplicity and efficient cache use (at the right block sizes), the \emph{naive} and \emph{idxvar} access strategies are thus generally among the fastest for all three stencils, even though (or because) they do not implement any advanced intricacies.

Surprisingly, for the grid described above, using the \emph{idxvar} strategy with the \emph{fastwaves} increased the overhead to $1.068x$ (from $1.044x$ in the \emph{naive} approach). As we later experimented with different grid storage strategies, such as \emph{non-chasing}, \emph{compressed} grids, the \emph{idxvar} strategy also turned out to not always be beneficial.

There are two likely explanations for the occasional advantage of the \emph{naive} variant: First, reading the same memory location a second time becomes almost as cheap as register access after it has been read for the first time, as it will be held in the L1 cache. Reaccessing the same memory location increases its chances of remaining in cache. As such, when a different thread requires the same memory location from the neighborship table, it is more likely to be in cache in the \emph{naive} variant due to its larger number of accesses.

Second, the \emph{naive} approach has better \emph{instruction-level parallelism}. Because the index variables are assigned at the start of an \emph{idxvar} thread, all neighborship table loads must be executed before anything else. Output calculations may only start once \emph{all} required neighborship pointers are loaded. In the \emph{naive} approach, on the other hand, neighborship pointer reads are not gathered at the beginning of the kernel. Instead, they are (re-)loaded at every point they are required in calculations. Thus, after one (or a few) neighbors have been loaded, useful intermediate calculations may already be performed, thus hiding some latency. Table \ref{tab:fastwaves-naive-idxvar-metrics} details profiler metrics that support these claims.

% main advantage idxvar vs naive: all neighbor pointers loaded together in the beginning; for non-chasing this is a disadvantage (all accesses bundled together; with naive some things can happen in between while load is happening); for chasing an advantage for idxvar (first load all level-one pointers, then all level two; re-loading all of this is costly for naive but has to be done because it can't know the first pointer hasn't changed in the meantime)


\subsection{\emph{Z-loop} and \emph{Z-loop-sliced} Access Strategies}

\begin{table}
	\begin{center}
    \begin{tabular}{l l p{2.5cm} p{2.5cm} p{2.5cm}}
        \hline
        \textbf{Stencil} & \textbf{Access} & \textbf{\texttt{achieved\_\allowbreak occupancy}} & \textbf{\texttt{ipc}} & \textbf{\texttt{dram\_\allowbreak read\_\allowbreak transactions}} \\
        \hline
        \hline
        \emph{laplap} & \emph{idxvar} & $0.947077$ & $0.812799$ & $11,946,136$ \\
        & \emph{z-loop} & $0.407226$ & $0.515881$ & $4,715,455$ \\
        & \emph{z-loop-sliced} & $0.421727$ & $0.638366$ & $4,729,444$ \\
        \hline
        \emph{hdiff} & \emph{idxvar} & $0.935448$ & $0.793701$ & $16,606,211$ \\
        & \emph{z-loop} & $0.363111$ & $0.481847$ & $9,330,226$ \\
        & \emph{z-loop-sliced} & $0.301747$ & $0.600179$ & $9,354,882$ \\
        \hline
    \end{tabular}
	\end{center}
    \caption{\label{tab:access-z-loop} Relevant metrics for the comparison of the \emph{z-loop} and \emph{idxvar} access strategies, on a $512\times 512\times 64$-sized grid (\emph{z-curves} memory layout with \emph{double} precision, \emph{uncompressed} and \emph{chasing} neighborship table) with $64\times 2\times 1$ threads. Observe that the \emph{z-loop} strategies effectively reuse neighborship pointers, leading to a lower number of device memory transactions, but suffer from a much lower occupancy and fail to use the full parallel computing capabilities of the GPU.}
\end{table}

The next obvious step was to try to make use of the Z-regularity of the grid. By loading the neighborship pointers once and reusing them for multiple Z-levels, we hoped for further improvements in runtime. 

However, for the \emph{hdiff} and \emph{fastwaves} stencils, the two \emph{z-loop} access strategies perform noticeably slower than the other available access strategies, across all tested grid storage configurations. The same is the case for the \emph{laplap} stencil for \emph{compressed} grids. The one exception is the \emph{laplap} stencil on an \emph{uncompressed} grid: In this case, the \emph{z-loop} access strategy is the fastest.

The hoped-for advantage of the two \emph{z-loop} access strategies is the reduced number of required neighborship table reads. Indeed, the number of reads to device memory is greatly reduced using this access strategy in all stencils (more than halved for \emph{laplap}, $56\%$ for \emph{hdiff}). In the case of the computationally simple \emph{laplap} stencil on an uncompressed grid, this reduced number of reads pays its dividends; the \emph{z-loop} access strategy is the fastest for this specific benchmark.

For other stencils (and the \emph{laplap} stencil in compressed grids), however, using the \emph{z-loop} access strategy is slower than all the alternatives.  We assume that this is due to the much lower (compared to other access strategies) occupancy of this loop-based access strategy. In the more computationally complex stencils, many latencies in the result computations may occur. To hide those latencies, a large number of threads are required. In the \emph{z-loop} access strategy, sequential processing of all elements with the same X- and Y-coordinates is prescribed by the code -- hindering parallel execution on the GPU. Presumably, this is less of an issue in the \emph{laplap} stencil due to its more simplistic result computation and fewer fields, which leads to fewer stalls in need of latency hiding.

In the \emph{z-loop-sliced} variant, we tried to address some of the low-occupancy issues by splitting the loop into smaller, parallelizable chunks. This approach tries to combine the best of both worlds: Reuse of neighborship table reads in a loop and more parallelism thanks to more threads. While the performance does improve in comparison with the \emph{z-loop} access strategy (confirming our theory that occupancy is indeed the bottleneck), it still falls short of the other access strategies due to the added overhead of managing a loop (which requires additional registers and a branch).

Table \ref{tab:access-z-loop} shows the relevant metrics of the described observations for an exemplary benchmark run.


\subsection{\emph{Shared} Access Strategy}

\begin{table}
    % ==20891== Profiling application: ./gridbenchmark --no-verify --size 512x512x64 --threads 64x1x8 hdiff-unstr-idxvar -z hdiff-unstr-idxvar-shared -z
    %    Kernel: void HdiffCudaUnstr::Chasing::hdiff_idxvar<double>(_coord3<int>, int const *, int, int, double const *, int const **, double const )
%         21                    tex_cache_transactions   Unified cache to Multiprocessor transactions    29954766    29968023    29962684
%         21                  l2_tex_read_transactions                L2 Transactions (Texture Reads)    22371588    22389494    22381872
%         21                  shared_load_transactions                       Shared Load Transactions           0           0           0
%         21                        tex_cache_hit_rate                         Unified Cache Hit Rate      72.14%      72.16%      72.15%
%         21                           l2_tex_hit_rate                              L2 Cache Hit Rate      50.48%      50.56%      50.53%
%         21                    dram_read_transactions                Device Memory Read Transactions     9358879     9369522     9362526
%    Kernel: void HdiffCudaUnstr::Chasing::hdiff_idxvar_shared<double>(_coord3<int>, int const *, int, int, double const *, int const **, double const )
%         21                    tex_cache_transactions   Unified cache to Multiprocessor transactions    26586311    26591481    26589268
%         21                  l2_tex_read_transactions                L2 Transactions (Texture Reads)    22323586    22341588    22333518
%         21                  shared_load_transactions                       Shared Load Transactions     5661664     5674694     5666573
%         21                        tex_cache_hit_rate                         Unified Cache Hit Rate      62.35%      62.38%      62.36%
%         21                           l2_tex_hit_rate                              L2 Cache Hit Rate      50.55%      50.63%      50.59%
%         21                    dram_read_transactions                Device Memory Read Transactions     9353607     9360039     9355089
	\begin{center}
    \begin{tabular}{l l l}
        \hline
        \textbf{Metric} & \textbf{\emph{idxvar}} & \textbf{\emph{shared}} \\
        \hline
        \hline
        \texttt{tex\_ cache\_ transactions} & $29,962,684$ & $26,589,268$ \\
        \texttt{shared\_ load\_ transactions} & $0$ & $5,666,573$ \\
        \texttt{dram\_ read\_ transactions} & $9,362,526$ & $9,355,089$ \\
        \hline
    \end{tabular}
	\end{center}
    \caption{\label{tab:access-shared} Number of transactions for the \emph{idxvar} and \emph{shared} access strategies at various levels of the memory hierarchy for a benchmark of the \emph{hdiff} stencil (grid of size $512\times 512\times 64$, \emph{z-curves} memory layout with \emph{double} precision, \emph{uncompressed} and \emph{chasing} neighborship table) at $64\times 1\times 8$ threads per block (optimal for both access strategies). Many L1 cache hits (\texttt{tex\_cache\_transactions}) in the \emph{idxvar} strategy are simply shifted to equally performant shared memory hits (\texttt{shared\_load\_transactions}) in the \emph{shared} strategy. The number of reads encountered at device memory is similar, further indicating that shared memory usage simply serves as an explicit, manually managed cache, taking the same role as the L1 cache in the \emph{idxvar} access strategy.}
\end{table}

As both the \emph{z-loop} and \emph{z-loop-sliced} variants had issues with occupancy, we explored the use of shared memory to pass neighborship information among cells that share the same neighbor offsets. This allows exploiting the Z-regularity of the grid while maintaining a large number of threads.

The performance of the \emph{shared} variant varies strongly depending on the type of storage of the neighborship tables (i.e. storage strategy) -- specifically, whether \emph{compression} for the neighborship tables is used or not.

In all scenarios with \emph{uncompressed} neighborship storage, the \emph{shared} access strategy performs almost identically to the \emph{idxvar} approach. A very slight overhead is observed for the \emph{shared} access strategy due to the required thread synchronizations. In this variant, the neighborship pointers loaded into shared memory appear to take the same role as the L1 cache. Thus, the \emph{shared} access strategy can be viewed as maintaining an \emph{explicitly managed cache}. In fact, as mentioned in section \ref{sec:memories}, shared memory and the L1 cache share the same physical memory. Table \ref{tab:access-shared} further evidences how the L1 cache is simply shifted to shared memory when using this scheme.

As we later discovered in our experiments with different grid storage strategies, grids stored using a \emph{z-curves}, \emph{non-chasing} and \emph{uncompressed} approach stand out in the \emph{shared} access strategy, as this is the only configuration in which the \emph{idxvar} scheme is slightly outperformed. With the aforementioned storage properties, explicit management of the unified L1/shared memory provides a minor benefit. This is probably due to the very large number of neighbor pointers in this storage strategy.

When using compressed neighborship tables, the \emph{shared} access strategy is noticeably slower than the \emph{idxvar} strategy. As there are relatively few neighborship pointers in compressed tables, those remain in \emph{L1} cache in the \emph{idxvar} variant permanently. In the \emph{shared} variant, those same (few) pointers must be explicitly (re-)loaded into shared memory for each thread block, which is less efficient.

% almost identical to idxvar for most storage strategies -> explicit cache management vs default idxvar cache TODO metrics shared memory accesses ~=~ cache hits?

% z-curves, non-chasing, uncompressed -> shared very slightly faster (there are really many neighborship pointers and explicitly using cache makes sense here (not enough reuse to make automatic cache useful))

% worse for compressed storage -> overhead of explicit shared memory management, possibly less efficient manual allocation?



\subsection{Summary}

Figure \ref{fig:storage-access} gives an overview of the possible storage/access-combinations and their performance. Table \ref{tab:overview} lists the fastest access strategy for all possible grid storage combinations.

No access strategy clearly dominates in all situations, but the \emph{idxvar} access strategy is often a good choice. Which implementation is fastest depends on the combination with the grid storage strategy, as well as on the stencil properties. Surprisingly, the \emph{naive} access strategy is competitive. It often performs similarly as the \emph{idxvar} strategy. In situations where a lot of pointer chasing occurs (\emph{chasing} grids and \emph{compressed} grids), using the \emph{idxvar} strategy is advantageous. We observed that the \emph{shared} access strategy behaves very similarly to the \emph{idxvar} strategy. We assume this is due to caching in the \emph{idxvar} strategy having the same effect as the explicitly used shared memory. The \emph{z-loop} and \emph{sliced z-loop} access strategies only gave an advantage in one configuration, namely for the \emph{laplap} stencil in grids stored in uncompressed (both chasing and non-chasing) fashion.

% Idea: percentage effect of access strategy given fixed storage; how important is access strategy?