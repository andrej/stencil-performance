\chapter{Conclusions}

Having established the basics of general-purpose GPU programming on the Nvidia CUDA platform in section \ref{sec:foundations}, we explored several approaches to storing unstructured grids and accessing them in the context of a stencil application in sections \ref{sec:grid-implementations} and \ref{sec:optimizations}. Concerning grid storage, we addressed the topics of \emph{coalescing}, \emph{pointer chasing} (neighbor-of-neighbor access), storage of multiple fields (array-of-structs or struct-of-arrays), and memory layout (\emph{row-major} and \emph{z-curves}), and evaluated the performance implications of decisions in all of those areas. Furthermore, we elaborated on a scheme that detects regular components and implemented neighborship table \emph{compression}. For grid access, we explored several ways to harness the regularity of the grid in the Z-dimension for better runtimes.

In section \ref{sec:results}, we assessed the performance of the implemented methods on three stencils of increasing complexity (\emph{laplap}, \emph{hdiff}, and \emph{fastwves}). Across all executed benchmarks, we measured that the cost of switching from a regular to an unstructured grid is a roughly constant time penalty influenced by the chosen access strategy and type of neighborship table storage. In the fastest optimized cases, the additional time required by the unstructured kernels was between $137 \mu s$ and $159 \mu s$ for the \emph{hdiff} and \emph{laplap} stencils on a $512\times 512\times 64$ grid, respectively. The overhead of the fastest implementation of the computationally more complex \emph{fastwaves} stencil was lower at only $102 \mu s$, supposedly because this stencil accesses fewer neighbors in the X-Y-plane.

In relation to regular grid runtimes, these numbers translate to relative slowdowns in the range of $1.04x$ to $1.45x$ for our fastest implementations. The biggest disparities in these relative slowdowns are observed due to differences between the stencils: As the absolute overhead is largely constant across all stencils, the cost of the required neighbor lookups carries more weight in simple stencils. For the most simple stencil, \emph{laplap}, relative slowdowns are largest with values between $1.45x$ and $1.71x$, depending on the chosen storage strategy for the neighborship tables. The more complex \emph{fastwaves} stencil, which accesses nine different fields (compared to the one field of the \emph{laplap} stencil) only suffers from slowdowns in the $1.04x$-area and is unaffected by the chosen storage strategy. The medium-intensity horizontal diffusion sits in the middle with its $1.25x - 1.52x$ range. 

Likely, the cost of switching to indirect addressing can be mainly attributed to the latencies introduced by the accesses to the neighborship tables. These lookups must occur before any stencil computations can be done; the measured overheads thus quantify these latencies.

Our proposed optimizations to neighborship table storage and access seem to show positive effects, as a comparison with the initial, naive-approach runtimes reveal: Using the \emph{naive} access strategy on an \emph{uncompressed} and \emph{chasing} neighborship table yields slowdowns of $2.07x$ for the \emph{laplap} stencil and $1.52x$ for the \emph{hdiff} stencil compared to regular grid execution times. This is $42\%$ and $22\%$ slower than respective optimized unstructured variants.

Table \ref{tab:overview} summarizes the fastest optimized storage/access-combinations for all stencils and reports their runtimes. We explored the differences that the employed storage and access strategies make in more detail in sections \ref{sec:res-storage} and \ref{sec:res-access}. For low- to medium-intensity stencils operating on large grids, we recommend using the \emph{idxvar} access strategy and storing the neighborship table in \emph{compressed, non-chasing} fashion, as this was the fastest combination across our benchmarks. 

We also observed that performance differences depend largely on the problem domain size. Most results presented focussed on grids of size $512\times 512\times 64$. In smaller grids, deliberate optimization attempts often proved to be ineffective. Some of our optimizations introduced small, one-time overheads intended to pay off later; this was only effective in larger-sized grids. Furthermore, the choice of the right number of threads when launching the kernels is especially important. Depending on the implementation, a different number of threads, or a different arrangement of the threads is better. In section \ref{sec:res-blocksize} the effect of the block sizes was evaluated and explained.

Executing unstructured grid applications on the \emph{CUDA} platform is especially promising for complex stencils, where we have shown that the overheads are relatively small in relation to the regular grid runtime ($1.04x$). While overheads are relatively seen larger in simple stencils ($1.25x - 1.45x$), we have also proven that effective improvements can be made as compared to naive approaches.