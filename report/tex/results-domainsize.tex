\section{Effect of Problem Domain Size and Precision} \label{sec:res-size}

This section gives some insight into how the performance of unstructured grid stencil computations relates to the input domain size. The relative overhead caused by unstructured grids varies strongly depending on the size of the grid. Generally, stencils applied to small unstructured grids are closer to regular grid runtimes than the same stencils applied to large unstructured grids (in relative terms). At smaller sizes, the cache can be used more effectively.

Of special interest are size changes in the Z-dimension only. Changing the Z-size and measuring the accompanying slowdowns reveals how well the different implementations are able to make use of the regularity in the Z-dimension. We investigate this in section \ref{sec:res-z-size-change}.

We also noticed that some grid \emph{storage optimizations} are only effective on grids of certain sizes. This is not addressed in this section, but in section \ref{sec:res-storage}.

\subsection{Change in X and Y Domain Size}

\begin{figure}
%     dat=pd.concat([large, medium, small])
% dat["rel"]=u.relmingroup(dat,by=u.col.size+u.col.stencil)
% fastest=u.groupmin(dat, by=u.col.problem+u.col.variant+u.col.storage)
%     u.barplot(fastest[fastest["unstructured"]&(fastest["size-z"]==64)&(fastest["stencil"]=="laplap")], cat=u.col.size, grp=u.col.variant, y="rel", tickrot=0)
	\begin{center}
    \includegraphics[scale=0.75]{laplap-z-curves-sizes.pdf}
	\end{center}
    \caption{\label{fig:variants-gridsizes} Relative runtime overhead compated to fastest regular variant of different implementations (access strategies) of the \emph{laplap} stencil on an unstructured grid with \emph{z-curves} memory layout. For each bar, the fastest storage variant is plotted (i.e. all combinations of \emph{compressed} and \emph{uncompressed}, \emph{chasing} and \emph{non-chasing} were benchmarked and fastest was plotted). Baseline: fastest regular grid implementation.}
\end{figure}

We have evaluated combinations of grid storage and access variants for the three stencils on grids of X-Y size $64\times 64$ (small grid), $128\times 128$ (medium grid) and $512\times 512$ (large grid). We observed that, relatively seen, increases in the X-Y-domain size affect the unstructured variants more negatively than the regular ones.

For small domains, the runtimes of unstructured variants are generally close to those of their regular counterparts. With increased domain size, there is an increase in the relative slowdown of the unstructured variant with respect to the fastest regular implementation. We assume this is due to cache sizes; while in smaller grids, entire neighborship tables and cell values remain cached, larger grids necessitate larger neighborship tables that cannot fit into the cache as a whole. Figure \ref{fig:variants-gridsizes} shows this trend for the \emph{laplap} benchmark, but the same characteristics apply to the other stencils as well.

The \emph{z-loop} grid access variant is the only exception to the described characteristics. Its relative slowdown decreases for larger grid sizes. This is probably due to two reasons. First, the z-loop variants already suffer from low occupancy in large grids. In small grids, occupancy is even lower, not making use of the complete parallelism capabilities of the GPU. Second, in small grids, the neighborship table may be in the cache in its entirety after one read. This allows subsequent neighborship table accesses to be practically free. This offsets the supposed optimization of the \emph{z-loop} variant, which aims to reduce the number of reads of neighborship table entries. This optimization only begins to provide a benefit in large grids.

\subsection{Change in Z domain size}
\label{sec:res-z-size-change}

\begin{figure}
    % dat = pd.read_csv("results/ultimate-reformat.csv")
    % dat["rel"] = u.relmingroup(dat, by=u.col.stencil+u.col.size)
    % u.lineplot(dat[(dat["stencil"]=="laplap")&(dat["variant"]=="idxvar")&dat["z-curves"]], x="size-z", y="rel")
% fig=u.plotdone(legend=0)
	\begin{center}
    \includegraphics[scale=0.75]{laplap-idxvar-z-sizes.pdf}
	\end{center}
    \caption{\label{fig:laplap-z-sizes} Relative slowdowns of the \emph{laplap} stencil on unstructured grids of various Z-sizes. Baseline: fastest regular grid implementation. The other two stencils show similar characteristics.}
\end{figure}

Given a $512\times 512$-sized base grid, we benchmarked our stencils with various Z-sizes ranging from $1$ to $64$. Analyzing the relative unstructured grid overhead, we observed two main patterns, which can be seen in figure \ref{fig:laplap-z-sizes}. Which pattern the overhead follows depends on the chosen grid storage. The observed patterns are as follows:

First, for uncompressed storage, the overhead decreases quickly for the first few Z-size steps, then fluctuates around some constant. This can be most clearly be seen in the \emph{non-chasing} stored grids. Second, when compression is used, the overhead increases in the first few Z-size steps, then also remains roughly constant.

These trends can be explained by two countering effects:

On one hand, the increased number of Z-levels naturally results in an increased total number of cells. In an unstructured grid, some extra effort has to be made to determine the location of each additional cell's neighbors. We call this the \emph{once-per-cell cost}. This effect alone would lead to constant additional relative costs (i.e. linear additional absolute costs) with respect to the regular grid.

On the other hand, as our grids are regular in the Z-dimension, neighborship pointers can be re-used among threads operating on cells with the same X- and Y-coordinates. Let us call a set of cells with identical X- and Y-coordinates a \emph{pillar of cells}. If neighborship pointers are effectively reused, loading them is only a \emph{once-per-pillar} cost. No matter how many Z-levels there are in the grid, the first load of the neighborship pointers remains a constant cost. As Z-size increases, this constant becomes vanishingly small in relation to the overall cost of the regular-grid computation. Thus, considering only the \emph{once-per-pillar} costs, performance should approach regular grid performance asymptotically for increasing Z. 

The observed relative overheads result from a combination of the two described effects (\emph{once-per-cell} and \emph{once-per-pillar} costs). In the uncompressed variants, the \emph{once-per-pillar costs} appear to dominate; thus, an increase in Z-size leads to better relative runtimes.

Some of our optimized access strategies aim to make explicit use of the \emph{once-per-pillar} nature of neighborship reads (\emph{shared}, \emph{z-loop} and \emph{z-loop-sliced}). Even the access strategies that do not explicitly exploit Z-regularity benefit from it if neighborship pointers happen to reside in caches.  In the best-case scenario, all neighborship pointers are already in the L1 cache (\emph{idxvar}), shared memory (\emph{shared} access strategy), or loaded into registers at the beginning of a loop (\emph{z-loop}) when a thread begins computation.

\subsection{Effect of Floating-Point Precision}

We observed no significant differences in the relative slowdown when using single-precision floating-point data types instead of double-precision data types. As expected, the absolute runtimes decrease with lower precision. This happens in the same proportions for both regular grid and unstructured grid implementations -- both are roughly $40\%$ faster than double-precision speeds.

As it appears to have no effect on the characteristics of our benchmarks, we consistently used double precision data types in all other experiments apart from this section.